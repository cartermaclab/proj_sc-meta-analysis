---
title             : "Meta-analytic findings of the self-controlled motor learning literature: Underpowered, biased, and lacking evidential value"
shorttitle        : "Meta-analysis of self-controlled research"
author: 
  - name          : "Brad McKay"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "1280 Main Street West, Ivor Wynne Centre Room AB-131 A1, McMaster University, Hamilton ON Canada, L8S 4K1"
    email         : "bradmckay8@gmail.com"
  - name          : "Zachary D. Yantha"
    affiliation   : "1"
  - name          : "Julia Hussien"
    affiliation   : "1"
  - name          : "Michael J. Carter"
    affiliation   : "2"
  - name          : "Diane M. Ste-Marie"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "School of Human Kinetics, University of Ottawa"
  - id            : "2"
    institution   : "Department of Kinesiology, McMaster University"

authornote: |
  \vspace{-3em}
  \addORCIDlink{Brad McKay}{0000-0002-7408-2323} \newline
  \indent\addORCIDlink{Julia Hussien}{0000-0001-7434-228X}  
  \addORCIDlink{Michael J. Carter}{0000-0002-0675-4271}  
  \addORCIDlink{Zachary D. Yantha}{0000-0003-1851-7609}  
  \addORCIDlink{Diane M. Ste-Marie}{0000-0002-4574-9539}
  
abstract: |
  The self-controlled motor learning literature consists of experiments that compare a group of learners who are provided with a choice over an aspect of their practice environment to a group who are yoked to those choices. A qualitative review of the literature suggests an unambiguous benefit from self-controlled practice. A meta-analysis was conducted on the effects of self-controlled practice on retention test performance measures with a focus on assessing and potentially correcting for selection bias in the literature, such as publication bias and *p*-hacking. First, a naïve random effects model was fit to the data and a moderate benefit of self-controlled practice, $g = .44\,(k = 52, N = 3134, 95\%\,CI\,[.31,\,.56])$, was found. Second, publication status was added to the model as a potential moderator, revealing a significant difference between published and unpublished findings, with only the former reporting a benefit of self-controlled practice. Third, to investigate and adjust for the impact of selectively reporting statistically significant results, a weight-function model was fit to the data with a one-tailed *p*-value cutpoint of .025. The weight-function model revealed substantial selection bias and estimated the true average effect of self-controlled practice as $g = .107\,(95\%\,CI\,[.047,\,.18])$. *P*-curve analyses were conducted on the statistically significant results published in the literature and the outcome suggested a lack of evidential value. Fourth, a suite of sensitivity analyses were conducted to evaluate the robustness of these results, all of which converged on trivially small effect estimates. Overall, our results suggest the benefit of self-controlled practice on motor learning is small and not currently distinguishable from zero.
  
keywords          : "Motor learning, retention, choice, ``OPTIMAL'' theory, meta-analysis, p-curve, publication bias"
# wordcount         : "X"
bibliography      : ["../refs.bib", "../r-references.bib"]
annotate_references: yes
nocite: |
  @Aiken2012-po,@Alami2013-gy,@Ali2012-rq,@Andrieux2016-lf,@Andrieux2012-dj,@Arsal2004-kf,@Barros2010-pc,@Barros2019-my,@Bass2015-cc,@Bass2018-vz,@Brydges2009-zb,@Bund2004-hy,@Carter2012-sj,@Chen2002-vg,@Chiviacowsky2014-ob,@Chiviacowsky2017-fy,@Chiviacowsky2002-ep,@Chiviacowsky2012-pk,@Chiviacowsky2008-bj,@Chiviacowsky2012-ri,@davis2009-dt,@Fagundes2013-ge,@Fairbrother2012-gz,@Ferreira2019-zo,@Figueiredo2018-kw,@Ghorbani2019-yz,@Grand2015-de,@Grand2017-de,@Hansen2011-rr,@Hartman2007-uv,@Hemayattalab2013-mq,@Ho2016-on,@Holmberg2013-qk,@Huet2009-wu,@Ikudome2019-ru,@Jalalvand2019-im,@Janelle1997-ht,@jones2010-qw,@Kaefer2014-bs,@Keetch2007-yp,@Kim2019-sl,@Leiker2016-zd,@Leiker2019-fz,@Lemos2017-qx,@Lessa2015-eq,@Lewthwaite2015-bd,@Lim2015-qs,@Marques2016-aw,@Marques2017-ue,@Norouzi2016-cx,@Nunes2019-nr,@Ostrowski2015-nb,@Patterson2010-tk,@Patterson2010-uu,@Patterson2013-nh,@Patterson2011-vt,@Post2016-vg,@Post2011-qc,@Post2014-yi,@Rydberg2011-zg,@Sanli2013-qh,@Ste-Marie2013-uc,@Tsai2015-rj,@Von_Lindern2017-al,@Williams2017-pj,@Wu2011-ha,@wu2007self,@Wulf2014-sn,@Wulf1999-pn,@Wulf2001-nb,@Wulf2018-ie,@Wulf2005-sz,@wulf2015-mg

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

# NEED TO BE RUNNING THE DEVELOPMENT BRANCH VERSION OF PAPAJA
documentclass     : "apa7"
classoption       : "man" # manuscript output
output            : papaja::apa6_pdf
header-includes   :
  - \usepackage{ragged2e}
  - \usepackage{pdflscape}
  - \pagewiselinenumbers
  - \raggedbottom
  #- \leftheader{\textbf{THIS PREPRINT HAS BEEN SUBMITTED FOR PEER-REVIEW}}
  #- \rightheader{\textbf{THIS PREPRINT HAS BEEN SUBMITTED FOR PEER-REVIEW}}
  #- \hypersetup{draft} # comment out after but right now will prevent error message about some link spanning multiple pages
  #- \usepackage{setspace}
  #- \AtBeginEnvironment{longtable}{\singlespacing}
  #- \AtBeginEnvironment{tablenotes}{\doublespacing}
  #- \captionsetup[table]{font={stretch=1.5}}
appendix:
  - "appendix_a.Rmd"
  - "appendix_b.Rmd"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(papaja)
library(kableExtra)
library(tidyverse)
# r_refs("../r-references.bib")
# Read in data files for tables
table_1 <- readr::read_csv("../revision/table-1.csv")
table_a1 <- readr::read_csv("../revision/table-a1.csv")
```

Asking learners to control any aspect of their practice environment has come to be known as self-controlled practice in the motor learning literature [@Sanli2013-xn; @Wulf2016-gf]. The first published experiments to test self-controlled learning asked learners to control their augmented feedback schedule [@Janelle1995-rj; @Janelle1997-ht]. For example, in an experiment by @Janelle1997-ht, participants practiced throwing tennis balls at a target with their non-dominant hand. The practice period occurred over two separate days. Participants were assigned to one of four experimental groups (*n* = 12): Self-controlled knowledge of performance, yoked-to-self-control, summary knowledge of performance after every five trials, and a knowledge of results only control group. The self-controlled group could request knowledge of performance whenever they wanted it, while each yoked group participant was matched with a self-control group counterpart and received knowledge of performance on the same schedule. The experimenter evaluated the participants’ throws, identified the most critical error in their throwing form, and provided knowledge of performance via video feedback, along with directing attention to the error and giving prescriptive feedback. During a delayed-retention test, the accuracy, form, and speed of the throw were assessed. The results indicated that the self-control group threw more accurately and with better form than all other groups on the retention test. The self-control and yoked groups did not significantly differ in throwing speed, but the control group threw faster than the self-control group on the second retention block. The results were interpreted as evidence that the participants provided with choice were able to process information more efficiently than their counterparts who received a fixed schedule of feedback.

(ref:fig1-caption) Number of self-controlled learning experiments meeting the inclusion criteria by year.

```{r fig1, echo = FALSE, fig.cap = "(ref:fig1-caption)", fig.align = "center", out.height = "51%"}
knitr::include_graphics("../../figs/fig1.pdf")
```

Figure \@ref(fig:fig1) shows that the number of experiments comparing self-controlled groups to yoked groups has been increasing since the original experiments by Janelle and his colleagues [-@Janelle1995-rj; -@Janelle1997-ht]. Researchers have experimented with giving learners control over a variety of variables in the practice environment. A qualitative assessment of the literature suggests that self-control is generally beneficial regardless of choice-type [@Wulf2016-gf]. For example, self-control has been effective when participants have been provided choice over what can be considered instructionally-relevant variables, such as: knowledge of results [@Patterson2010-tk], knowledge of performance [@Lim2015-qs], concurrent feedback [@Huet2009-wu], use of an assistive device [@Wulf2001-nb], observation of a skilled model [@Lemos2017-qx], practice schedule [@Wu2011-ha], practice volume [@Lessa2015-eq], and task difficulty [@Leiker2016-zd]. Additionally, self-controlled benefits have also been found for instructionally-irrelevant variables, such as: the colour of various objects in the practice environment [@Wulf2018-ie], other decorative choices [@Iwatsuki2019-tf], and the choice of what to do after the retention test is complete [@Lewthwaite2015-bd].

Despite the widespread optimism that self-controlled practice is useful for enhancing motor learning, researchers continue to debate the underlying mechanisms responsible for the effect [@Carter2017-mk; @Wulf2018-ie]. Beginning with @Janelle1995-rj, both motivational and information processing mechanisms were proposed as possible explanations for self-control benefits. Researchers have since supported these two mechanisms and, from a motivational perspective, have posited that self-control enhances confidence [@Janelle1995-rj; @Chiviacowsky2012-cw; @Wulf2016-gf] and satisfies the basic psychological need for autonomy [@Sanli2013-xn; @Wulf2016-gf], motivating motor performance and learning enhancement. Most self-controlled learning experiments, however, have involved participants making choices over potentially informative variables, which could act as a confounding variable. Citing this potential motivational/informational confound, @Lewthwaite2015-bd experimented with providing instructionally-irrelevant choices, such as the colour of the golf balls to putt, the painting to hang on the wall, and what to do following the retention test. Lewthwaite and her colleagues reasoned that information processing explanations could not account for benefits due to these incidental choices, and instead motivational factors would be more likely. Consistent with the motivational hypothesis, participants exhibited significantly greater motor learning on a golf putting task (Experiment 1) and on a balance task (Experiment 2). Subsequently, several experiments have reported benefits with instructionally-irrelevant choices [@Abdollahipour2017-dr; @Chua2018-sn; @Halperin2017-az; @Iwatsuki2019-tf; @Wulf2014-sn; @Wulf2018-ie], further reinforcing this motivational perspective.

A contrasting line of research has been reported by Carter and his colleagues [-@Carter2014-up; -@Carter2017-ix; -@Carter2017-mk] in which informational factors, the second dominant perspective, are given more weight as an explanatory variable. In one experiment by @Carter2014-up, self-control participants were provided with choice over receiving knowledge of results, but divided into three experimental groups; those who could make their knowledge of results decision: before the trial, after the trial, or both (they would decide before, but could change their mind following the trial). Timing of the choice significantly attenuated the self-control benefit. While the self-after and self-both groups exhibited learning advantages relative to their yoked counterparts, the self-before group displayed no such advantage. The argument proffered by the researchers was that there was more informational value to be gained from knowledge of results requested after a trial than when it had to be requested before the outcome of the trial occurred [also see @Chiviacowsky2005-iq].

In another experiment [@Carter2017-ix], asking learners to complete an interpolated activity in the interval preceding their choice of whether to receive knowledge of results significantly attenuated the self-control benefit [also see @Couvillion2020-zw; @Woodard2020-uq]. As a final example, @Carter2017-mk compared an instructionally-relevant choice group (i.e., when to receive knowledge of results) to an instructionally-irrelevant choice group (i.e., which video game to play after retention and which colour arm wrap to wear while practicing). Unlike the experiment by Wulf and colleagues [-@Wulf2018-ie], Carter and Ste-Marie found that instructionally-relevant choices were more effective than task-irrelevant choices. Overall, they have used these different findings to tie self-controlled learning benefits to information-processing activities of the learner and, in particular, those related to the processing of intrinsic feedback [e.g., @Carter2017-ix; @Chiviacowsky2005-iq] and the provided knowledge of results [e.g., @Grand2015-de].

In the present research, these different viewpoints concerning the mechanisms of self-controlled learning advantages were examined via meta-analysis with choice-type included as a moderator. The logic was that the motivational and informational perspectives would have different predictions. More specifically, from a motivation hypothesis, no moderating effect of choice-type on motor learning would be expected. In contrast, smaller effects for irrelevant-choice type, as compared to relevant-choice types, would be expected from the information-processing perspective.

Beyond this interest in the possible theoretical mechanisms, a more important question addressed was whether there is in fact evidential value for the self-controlled learning benefit. This is of relevance because the current consensus in the field is that self-controlled practice is generally more effective than yoked practice [for reviews see @Sanli2013-xn; @Ste-Marie2019-bw; @Wulf2016-gf]. Reflecting this confidence in its benefits for motor learning, researchers have recommended adoption of self-control protocols in varied settings, such as medical training [@Brydges2009-zb; @Jowett2007-et; @Wulf2010-uw], physiotherapy [@Hemayattalab2013-mq; @Wulf2007-nc], music pedagogy [@Wulf2008-de], strength and conditioning [@Halperin2018-rs], and sports training [@Janelle1995-rj; @Sigrist2013-ux].

Problematic though is that recent, high-powered experiments with pre-registered analysis plans have failed to observe motor learning or performance benefits with self-control protocols [@Grand2017-de; @McKay2020-vj; @Yantha2019-dt; @StGermain2021-lg]. Against the backdrop of the so-called replication crisis in psychology [@Open_Science_Collaboration2015-ay], there is reason for pause when evaluating the ostensible benefits of self-controlled learning. Further, @Lohse2016-cf have raised concerns about publication bias, uncorrected multiple comparisons, *p*-hacking, and other selection effects in the motor learning literature. Therefore, to address the impact of selection effects on estimates of the self-controlled learning effect, a weight function model [@Carter2019-vv; @Hedges1996-yh; @Vevea1995-uv; @Vevea2005-pv; @McShane2016-by] with a one-tailed *p*-value cutpoint of .025 was fit to the dataset of effects to provide a pre-registered adjusted estimate of the overall self-controlled learning effect. Even the adjusted estimate is biased if the data generating processes are biased in ways not captured by the assumptions of the model, so further sensitivity analyses were conducted to estimate the average effect of self-control after correcting for selection effects [@Carter2019-vv; @Vevea2005-pv]. In parallel, in an effort to investigate the presence of evidential value in the literature, significant results were subjected to a *p*-curve analysis [@Simonsohn2014-yo; @Simonsohn2015-mn]. The *p*-curve analysis focuses exclusively on significant results and therefore is not affected by publication bias.

In sum, the objectives of this meta-analysis were to estimate the true average effect of self-controlled learning and evaluate the evidential value of the self-controlled learning literature. Bias resulting from selective publication was addressed with weight function and *p*-curve models and effect size estimates were adjusted accordingly. A key theoretical question \textcolor{red}{related to the underlying mechanisms of putative self-controlled learning advantages (motivational versus informational influences)} was also addressed through moderator analyses, but, to anticipate, inferences will depend on the reliability of the evidence overall. Finally, sensitivity analyses were conducted in addition to pre-registered analyses in an effort to understand the extent that our conclusions depended on the modeling techniques and assumptions adopted.

# Method

## Pre-registration
The procedures followed to conduct this meta-analysis were pre-registered and can be viewed at https://osf.io/qbg69 \hyperref[sec:sharing]{(see Data, materials, and code availability section)}. This meta-analysis was retrospective and earlier samples of the literature had been meta-analyzed prior to this pre-registration, albeit with different data collection procedures, scope, and excluding recent experiments. \textcolor{red}{This study adheres to PRISMA reporting guidelines} [@page2021].

## Literature search
The literature search and data extraction were conducted by three authors (BM, ZY, JH) and one research assistant (HS) independently. The goal of the search was to identify all articles that met the inclusion criteria for the meta-analysis. Specifically, randomized experiments were subject to five criteria for inclusion: 1) A self-control group in which participants were asked to make at least one choice during practice, 2) a yoked group that experienced the same practice conditions as the self-controlled group, 3) a delayed ~24-hour retention test or test with longer delay interval, 4) an objective measurement of motor performance, and 5) publication in a peer-reviewed journal or acceptance as part of a Master’s or PhD thesis. \textcolor{red}{The literature search was completed on August 2, 2019.}

The search commenced at PubMed and Google Scholar with the following query: *self-control** OR *self-regulat** OR *self-direct** OR *learner-control** OR *learner-regulat** OR *learner-direct** OR *subject-control** OR *subject-regulat** OR *subject-direct** OR *performer-control** OR *performer-regulat** OR *performer-direct** AND *motor learning**. The query retrieved 9014 hits on PubMed and 98,600 hits on Google Scholar. Each researcher excluded hits based on title alone or title and abstract when necessary, and quit searching the databases at self-selected intervals following extended periods of excluding 100\% of search results. Following an initial run of searching databases, each researcher employed their own search strategies, including reviewing the reference sections of reviews and included articles, consulting the "OPTIMAL" theory website[^1], and searching the ProQuest Thesis database.

[^1]: The webpage link that was consulted (https://optimalmotorlearning.com/index.php/did-you-know-that/) is no longer available. A new webpage devoted to ``OPTIMAL" theory can be accessed using the following link: https://gwulf.faculty.unlv.edu/optimal-motor-learning/.

This literature search process resulted in 160 articles that could not be excluded without consulting the full-text of the article. All 160 articles were coded for inclusion or exclusion by two researchers independently. All instances of disagreement between coders were reviewed by three authors (BM, ZY, and JH), and consensus was reached in each case. Disagreements were infrequent and were often caused by a lack of clarity in the articles (e.g., 100% knowledge of results groups labeled as yoked groups). None of the coding disagreements evolved into conceptual disagreements. Rather, in each case, it was identified that one coder had missed a detail in the full text that changed its inclusion eligibility. Subsequent to this process, a total of 73 articles, which included 78 experiments, met the inclusion criteria (see Table \@ref(tab:table1)).

## Dependent variable selection
The focus of this meta-analysis was on performance outcomes associated with the goal of the skill. The primary theoretical perspectives offered as an account for self-controlled learning are likewise focused on performance outcomes. For example, the "OPTIMAL" theory proposes that a learner's movements become coupled with the goal they are trying to achieve when they experience autonomy-support during practice [@Wulf2016-gf]. To reflect this focus, a dependent measure priority list was developed that gave higher priority to absolute error measures and less priority to consistency measures, time/work measures, and form scores. Dependent measure priority was ordered as follows: 1) absolute error (and analogous measures: radial error, points in an accuracy measure), 2) root-mean-square-error (RMSE), 3) absolute constant error, 4) variable error, 5), movement time (and distance travelled), 6) movement form – expert raters, 7) otherwise unspecified objective performance measure reported first in research report.[^2] In the event that multiple measures of motor performance were reported for an experiment, effect sizes were calculated for the highest priority measure reported in the study. In experiments with multiple self-control groups and one yoked group, the self-control groups were combined [@Higgins2011-rg]. If multiple choice-types or sub-populations were included in an experiment, combined and individual effects were calculated for inclusion in moderator analyses.

[^2]: Radial error, accuracy points, and distance travelled were added to the pre-registered dependent measures as they arose during data-extraction. Decisions were made blind to the data by an author not involved in said extraction (BM or DSM).

Many of the self-controlled learning experiments analyzed in this study included multiple dependent measures. However, including multiple measures from the same experiment introduces bias and inflates \textcolor{red}{Type 1 error} [@Scammacca2014-do]. Although there are a variety of methods for dealing with multiple measures from the same studies in meta-analysis, we chose to create a priority list and always selected the highest priority dependent measure that was reported. If the highest priority measure was not described in adequate detail to calculate the effect size, the authors were contacted and the data were requested. If the authors could not provide the data for the highest priority dependent measure reported in their study, the experiment was left out of our analysis.

The rationale for selecting the approach we did was based on five considerations. First, our interest was in motor learning as reflected by an enhanced capability to perform a skill. Motor learning studies often report multiple error measures, but they are not equally coupled with performance outcome. Constant error, for example, was not included on the priority list because it is possible to have zero constant error while performing terribly overall. Therefore, we chose to prioritize measures that could be considered to be tightly coupled with performance, like absolute error, RMSE, and absolute constant error. If these measures were not used, measures that are only correlated with performance, such as variable error, movement time, and movement form, were selected. We reasoned this selection strategy would focus the analysis on measures related to improved skill while de-emphasizing other effects. Second, we reasoned that averaging across dependent measures could introduce additional heterogeneity to the analysis by including potentially disparate dependent measures. The third, fourth, and fifth considerations all relate to avoiding bias but differ with regard to the source of the bias and the alternate method that would include such bias. Thus, the third consideration was that imposing a priority list was thought to better avoid biases that could emerge from selecting the most focal measure in a given study, because an unknowable percentage of studies may have defined the focal measure based on the strength of the findings. Fourth, we reasoned that some measures may only get reported if they support the predicted benefit of self-control. @Scammacca2014-do reported that effect size estimates were inflated when random dependent measures were selected in a meta-analysis case study, perhaps reflecting a selective reporting bias. Averaging across all reported measures--a fair alternative to our approach--could conceivably pick up some of this reporting bias. Fifth, we ignored lower priority measures with data when higher priority measures lacked data because we reasoned there could be a systematic reason for this pattern: preference for reporting data associated with positive effects. Indeed, there were articles where the only measure reported with sufficient data to calculate an effect size was also the only measure with a significant result [e.g., @Wulf2005-sz].

```{r table1, echo=FALSE, results="asis"}
kbl(table_1,
    longtable = TRUE,
    booktabs = TRUE,
    escape = TRUE,
    linesep = "\\addlinespace",
    caption = "Experiment characteristics and moderator coding."
    ) %>%
  landscape() %>%
  kable_styling(position = "left",
                #latex_options = "repeat_header",
                font_size = 10) %>%
  footnote(general = "KR = Knowledge of results; KP = Knowledge of performance.",
           general_title = "Note.",
           threeparttable = TRUE)
```

## Data extraction
The four researchers separated into pairs and half of the included experiments \textcolor{red}{were coded independently by one pair. The other half were coded independently by the other pair.} The coding included varied moderators, publication year, and sample size. Also Hedges’ *g* was calculated from reported statistics and sample size using the `compute.es` package [@R-computees] in `R` [@R-base]. Effect sizes were calculated from means and standard deviations, test statistics like *t* and *F*, or from precisely reported *p*-values. When covariates were included in the analysis, the correlation coefficient for the covariate - dependent measure relationship was required to calculate accurate effect sizes. Since this information is often not reported, authors were contacted and the information was requested. One effect size was calculated for each of three time points for each experiment: Acquisition, retention, and transfer.

The independent data extractions were compared and inconsistent results were highlighted. There was 89\% absolute agreement between pairs of coders on 1344 data points. For those with disagreement, one of the researchers from the other coding pair reviewed the relevant experiment to confirm the value to be used in the analysis.[^3]

[^3]: On one occasion, the third researcher was unable to match either effect calculation, so the involved researchers discussed the issue, determined the source of the inconsistency, and asked a fourth researcher to recalculate the effect size with clear instructions for avoiding confusion. The source of inconsistency was simply a rounding error when combining multiple groups and the fourth researcher was able to corroborate the calculation.

Several articles failed to report the data necessary to calculate effect sizes at some or all time-points. A total of 39 authors were emailed with requests for missing data and 17 were able to provide data following a minimum one month period following the request. After requesting missing data, 25 experiments were excluded from primary analyses for missing retention data. A total of 52 effects from 51 experiments reported in 46 articles were included in the primary meta-analysis.

In addition to extracting effect sizes, inferential statistics were scraped from published experiments that reported a statistically significant effect at retention. Two authors (BM and JH) independently completed a *p*-curve disclosure form consisting of a direct quote of the stated hypotheses for each experiment, the experimental design, and a direct quote of the results indicating a significant result **(see Appendix A)**. There was 94% absolute agreement between the independent forms. Mismatches were resolved with consensus.

## Outlier screening
The meta-analysis `R` package `metafor` [@R-metafor] was used to screen the data for potentially influential outliers (see analysis script). In order to identify outlier values and exclude them from further analyses, the following nine influence statistics were calculated: a) externally standardized residuals, b) DFFITS values, c) Cook's distances, d) covariance ratios, e) DFBETAS values, f) the estimates of $t^2$ when each study is removed in turn, g) the test statistics for (residual) heterogeneity when each study is removed in turn, h) the diagonal elements of the hat matrix, and i) the weights (in %) given to the observed outcomes during the model fitting. Any experiment with effects identified as extremely influential by any three of the influence metrics were removed from subsequent analyses.

## \textcolor{red}{Risk of bias}
\textcolor{red}{All articles were assessed for risk of bias by the lead author using the Cochrane Risk of Bias 1.0 tool} [@higgins2011]. \textcolor{red}{Each article was coded as either high risk, unclear (some concerns), or low risk on 7 dimensions: sequence generation, allocation concealment, incomplete outcome data, selective outcome reporting, blinding of outcome assessment, blinding of participants and personnel, and other sources of bias.}

# Pre-specified analyses

## Random effects model
A naïve random effects model was fit to the retention effect sizes to estimate the average reported effect of self-controlled learning and to assess heterogeneity in effect sizes between experiments. Heterogeneity was evaluated with the *Q* statistic and described with $I^2$. A mixed-effects model was fit to evaluate whether differences in experimental design or sample characteristics moderated the effect of self-controlled learning.

## Moderator analyses
Moderators were determined based on the authors’ collective knowledge of the self-controlled learning literature. We coded for discrete differences in protocols between experiments to investigate whether differing methodologies resulted in different effect size estimates. Further, based on a meta-analysis reporting that the effect of choice on intrinsic motivation can be moderated by whether participants were compensated for completing the study [@Patall2008-oy], we also coded for compensation type. Finally, we investigated whether publication status was a moderator of the effect of self-control as part of our overall approach to examining the impact of publication bias on the self-controlled learning literature. The following six moderators were analyzed separately in mixed-effects models: a) *Choice-type*: Choices were categorized as either instructionally-irrelevant, knowledge of results, knowledge of performance, concurrent feedback, amount of practice, use of assistive device, practice schedule, observational practice, or difficulty of practice; b) *Experimental setting*: Experiments were categorized as either laboratory, applied, or laboratory-applied. We defined a laboratory setting as one where learners are asked to acquire a skill not typically performed in everyday life. We defined an applied setting as one where learners are asked to acquire a skill often performed outside of a laboratory. Finally, we defined a laboratory-applied setting as one where learners are asked to acquire a skill resembling skills often performed outside the laboratory but with researcher-contrived differences; c) *Sub-population*: The following subgroups were analyzed: Adult (18-50 years of age), children/adolescents (under 18-years old), older adult (over 50-years-old), and clinical (clinical population defined by the research article); d) *Publication status*: Articles were classified as published or unpublished (e.g., theses); e) *Compensation*: Whether participants were compensated for participating in the experiment was categorized as compensated, not compensated, or not stated; f) *Retention delay-interval*: Coded as 24-hour, 48-hours, or >48-hours.

## Adjusting for selection effects
Selection bias in the motor learning literature is likely caused by filtering based on the statistical significance of results [@Lohse2016-cf]. To assess and adjust for selection effects, the `R` package `weightr` [@Coburn2017-hv]  was used to fit a Vevea-Hedges weight function model to the retention data [@Vevea1995-uv]. The weight-function model estimates the true average effect, heterogeneity, and the probability that a non-significant result \textcolor{red}{survives censorship and is available for analysis}. Selection effects are modelled by a step function that divides the effects into two bins at one-tailed *p* =.025, coinciding with a two-tailed *p*-value of .05. The probability of a non-significant effect \textcolor{red}{surviving censorship to appear} in the model is estimated relative to the probability of observing a study with a significant effect. The selection-adjusted model was compared to the naïve random effects model with a likelihood ratio test. Better fit from an adjusted model suggests selection bias in the literature.

The adjusted estimate from the weight-function model was pre-registered as the primary estimate of the true average effect in this meta-analysis. Please note that while the weight-function model attempts to estimate the true effect of self-controlled learning after correcting for selection biases, the estimated effect cannot be considered definitive. Nevertheless, the adjusted estimate is likely less biased than the naïve random effects estimate [@Carter2019-vv; @hong2021; @kvarven2020; @Vevea1995-uv]. The difference between the estimates can be informative about the potential impact of selection biases, with larger disparities between models suggesting greater selection effects.

## *P*-curve analysis
To investigate the evidential value of the self-controlled learning literature, the significant positive results at retention reported in peer-reviewed journals were submitted to a *p*-curve analysis [@Simonsohn2015-mn]. To be included in the analysis, articles needed to meet the following criteria: a) be a published article; b) state explicitly that self-controlled learning was expected to be more effective than yoked practice; c) report inferential statistics comparing a self-control group and a yoked group directly on a retention test; d) conclude that the self-control group performed significantly better than the yoked group. If the article included multiple dependent measures showing a significant effect, the dependent measure priority list was used to select the highest priority measure. If only one measure was reported as significant, that effect was included even if the experiment included higher priority measures that were null. This resulted in a slightly different sample of effects from the random effects and weight-function models.

The distribution of significant *p*-values is a function of the power of the experiments included in the analysis. If a *p*-curve included only Type 1 errors, the expected distribution would be uniform. As the power of included experiments increases, so too does the amount of right skew in the *p*-curve, with smaller *p*-values appearing more frequently than large *p*-values. The *p*-curve analysis tests the null hypothesis that there is no evidentiary value by analyzing the amount of right skew in the distribution of *p*-values. Conversely, if researchers peek at their data and stop collecting when they reach statistical significance, a practice known as *p*-hacking, the distribution of significant *p*-values under the null would be left skewed, with *p*-values near .05 occurring more frequently. Varying mixtures of true effect sizes and intensities of *p*-hacking produce varying shapes of *p*-curve, therefore the observed *p*-curve was compared to the distribution of *p*-values expected if the studies were conducted with 33% power. It is unlikely that researchers would continuously conduct experiments that fail >66% of the time whilst studying the self-controlled learning phenomenon. Observing a *p*-curve significantly ``flatter'' than what would be expected with 33% power would suggest a lack of evidential value among the significant results [@Simonsohn2014-xq; @Simonsohn2014-yo].

# Sensitivity analyses
The primary analyses were followed up with several sensitivity analyses. Sensitivity analyses are used to evaluate the sensitivity of the results to the specific parameters chosen for the original analyses. The self-controlled learning literature, like many areas of behavioural research, was not produced exclusively by registered experiments with pre-specified analysis plans and 100% reporting frequency. The complexity of selection effects at various levels, including editorial decisions, author decisions, analysis decisions, and missing data, renders the accuracy of modeled effects impossible to estimate [@Carter2019-vv]. Producing a range of estimates based on varying assumptions is intended to provide the reader with a broader picture of the uncertainty of the point estimates in the primary analyses.

Bias correction methods vary in their performance depending on the total amount of heterogeneity, the true average effect size, the amount of publication bias, and the intensity of *p*-hacking in the data [@Carter2019-vv]. To determine which bias correction models perform well in the various plausible conditions for data in this meta-analysis, model performance checks were conducted using the \href{http://www.shinyapps.org/apps/metaExplorer/}{Meta-Showdown Explorer} shiny app developed by Carter and colleagues [-@Carter2019-vv]. Simulated conditions were as follows: Medium publications bias (significant results published at 100\% frequency, non-significant published at 20\% frequency, wrong direction effects published at 5\% frequency), \textcolor{red}{medium questionable research practice environment (QRP; see Carter et al.} [-@Carter2019-vv] \textcolor{red}{for detailed explanation of QRP environment)}, $\tau$ = 0, .2; $g$ = 0, .2, .5; $k$ = 60, good performance defined as a maximum of .1 upward or downward bias, and maximum mean absolute error of .1, also tested with maximum bias and error values of .15. With good performance defined by a maximum bias in either direction of .1 and maximum absolute error of .1, the weight function model and, \textcolor{red}{to a lesser extent,} *p*-curve models provided coverage across all plausible conditions except the highest heterogeneity condition ($\tau$ = .4). With good performance defined as a maximum bias and error of .15, the precision-effect with standard error (PEESE) method provided good performance in all conditions. Therefore, sensitivity analyses were conducted on effect size data via *p*-curve and PEESE methods. \textcolor{red}{An additional sensitivity analysis of the estimated power among included studies was conducted with the \emph{z}-curve [@bartovs2020]. \emph{Z}-curve, like \emph{p}-curve, analyzes only statistically significant results and estimates the power of the included studies (called expected replication rate, ERR). However, unlike \emph{p}-curve, \emph{z}-curve is robust to heterogeneity because it fits a finite mixture model of seven distributions, allowing the underlying true effects to vary. Further, \emph{z}-curve also estimates the power of all studies that have been conducted (called expected discovery rate, EDR) which can be compared to the observed discovery rate in order to test for the presence of publication bias.}

## Primary *p*-curve
A leave-one-out analysis of *p*-curve results was conducted to assess the extent to which the primary results depended on the inclusion of one or two extreme results. Results that depend on the inclusion of one or two extreme results should not be considered robust.

# Results

## \textcolor{red}{Risk of bias}
\textcolor{red}{The risk of bias assessment revealed lackluster reporting standards were pervasive among the included articles} (see Figure \@ref(fig:fig2)). \textcolor{red}{For example, comparing a self-control group to a yoked group usually involves first collecting a self-control participant, then their yoked counterpart. Despite this, most articles simply reported that the participants were randomly assigned to these conditions, with no indication of how this temporal constraint was addressed. A similar issue was observed with respect to addressing outliers and attrition. Over 75\% of the included articles failed to mention outliers and how they were addressed (captured by the incomplete outcome data dimension). Most studies included in this study were not double-blind, largely due to the inherent difficulties in conducting a double-blind study of self-controlled motor learning. While the risk of bias associated with a lack of double blinding has been debated} [see @howick2008], \textcolor{red}{it is nonetheless notable that double-blinding was rare among the included studies.} 

(ref:fig2-caption) \textcolor{red}{Proportion of studies with low risk, some concerns, and high risk of bias in each of the seven dimensions of the Cochrane RoB 1.0 tool.}

```{r fig2, echo = FALSE, fig.cap = "(ref:fig2-caption)", fig.align = "center", out.height = "81%"}
knitr::include_graphics("../../figs/fig2.pdf")
```

## Outlier removal
Two studies were flagged as significantly influential outliers by all nine influence metrics calculated during data screening: @Lemos2017-qx, *g* = 3.7, and @Marques2017-ue, *g* = 3.95. No other effect sizes were identified as outliers by any metric. Both outliers were removed from all subsequent analyses.

(ref:fig3-caption) Forest plot of Hedges' *g* (95% CI) for self-controlled versus yoked groups on retention tests. Size of squares is proportional to $1/\sigma^2$ (precision). Light grey polygons represent \textcolor{red}{95\% $CI$ estimates from publication-status moderator analysis. Estimates from unpublished studies center on $g$ = .003 and published studies on $g$ = .54}

```{r fig3, echo = FALSE, fig.cap = "(ref:fig3-caption)", fig.align = "center", out.height = "81%"}
knitr::include_graphics("../../figs/fig3.pdf")
```

## Naïve random effects model
The naïve random effects model estimated the average treatment effect of self-controlled practice, *g* = .44 (*k* = 52, *N* = 3134, 95% *CI* [.31, .56]).  However, there was significant variability in the average effect estimated across experiments, *Q*(*df* = 51) = 103.45, *p* < .0001, $\tau$ = .31. It was estimated that 47.9% ($I^2$) of the total variability in effect sizes across experiments was due to true heterogeneity in the underlying effects measured (see Figure \@ref(fig:fig3)). 

## Moderator analyses
Six moderators selected for theoretical and/or methodological reasons were tested separately. Five moderators failed to account for a significant amount of heterogeneity: experimental setting (*p* = .46, $R^2$ = 1%), compensation (*p* = .99, $R^2$ = 0%), choice-type (*p* = .71, $R^2$ = 0%), sub-population (*p* = .74, $R^2$ = 0%), and retention interval (*p* = .54, $R^2$ = 0%). One moderator, publication status, accounted for a statistically significant amount of heterogeneity, *p* < .0001, $R^2$ = 48%. Among published experiments, self-controlled practice had a strong benefit, *g* = .54, 95% *CI* [.28, .81]. However, among unpublished experiments, self-controlled practice had essentially no effect, *g* = .003, 95% *CI* [-.23, 24].

## Selection model
The weight-function model combines an effect size model and a selection model [@Hedges1996-yh]. The effect size model is equivalent to the naïve random effects model, specifying what the distribution of effect sizes would be in the absence of publication bias or other selection effects. The selection model accounts for the probability a given study survives selection based on its *p*-value and specifies how the effect size distribution is modified by selection. A weight-function model with a *p*-value cutpoint of (one-tailed) .025 was fit to the retention effect size estimates(see Figure \@ref(fig:fig4)). The results of a likelihood ratio test suggest the adjusted model was a significantly better fit to the data than the unadjusted model, $\chi^2$(*df* = 1) = 21.18, *p* < .0001.[^4] The adjusted effect size estimate was significantly different from zero, *g* = .107, *p* < .001, 95% *CI* [.05, .17]. According to the adjusted model, non-significant results were 6% as likely to survive selection as significant results. \textcolor{red}{Note that the \texttt{weightr} function failed to estimate the random effects model and the results reported here are based on a fixed-effect estimate.}

[^4]: Be aware that the likelihood ratio test is not robust to misspecification of the random effects model [@Hedges1996-yh].

(ref:fig4-caption) Funnel plot of self-controlled learning studies at retention. Standard error is plotted on the y-axis and Hedges' g is plotted on the x-axis. \textcolor{red}{Dark gray contour regions represent two-tailed p-values between .10 and .05 (not quite significant). The light gray contour regions represent two-tailed p-values between .05 and .01.} In the absence of bias (and other forms of heterogeneity), the most precise experiments would centre on the naïve random effects estimate near the \textcolor{red} {top} of the plot and as experiments get progressively less precise they would move \textcolor{red}{down} the plot and spread out symmetrically. In the presence of bias, one would expect experiments to \textcolor{red} cluster in the light gray contour regions. The clustering of experiments \textcolor{red}{in the positive light gray  contour region} in the above plot suggests substantial bias.

\vfill

```{r fig4, echo = FALSE, fig.cap = "(ref:fig4-caption)", fig.align = "center"}
knitr::include_graphics("../../figs/fig4.pdf")
```

## *P*-curve
The purpose of the *p*-curve analysis was to investigate the evidential value in the published reports (*N* = 26) of statistically significant self-controlled learning benefits. Visual inspection of Figure \@ref(fig:fig4) reveals a v-shaped distribution with the greatest frequency of *p*-values in the <.05 bin. The observed *p*-curve was significantly flatter than would be expected if the experiments had 33% power, *p* = .0035, indicating an absence of evidential value. Conversely, the half *p*-curve [@Simonsohn2015-mn] was significantly right skewed, suggesting the presence of evidential value. Sensitivity analysis, however, revealed that the half curve does not remain significantly right skewed following removal of the most extreme *p*-value from the sample. \textcolor{red}{The estimated power of the included studies was 5\%, 95\% $CI$ [5\%, 17\%].}

## Interim discussion
The primary results described above suggest that selection effects have caused a seriously distorted record of self-controlled learning. Estimated benefits are less than one third of the naïve estimate, *g* = .107, 95% *CI* \textcolor{red}{[.05, .17]}. The *p*-curve analysis failed to detect robust evidence of a self-controlled learning effect. The performance of the weight-function model depends on the specific conditions present in the meta-analysis, although these conditions are unknowable [@Carter2019-vv]. It was necessary to conduct sensitivity analyses with additional bias correction methods to assess the reliability of the selection-adjusted weight-function model estimate. Based on performance checks conducted under a range of plausible conditions, it was determined that sensitivity analyses conducted with a PEESE meta-regression and *p*-curve effect size estimation would provide good performance coverage across most plausible conditions.

(ref:fig5-caption) *P*-curve analysis of published experiments that were statistically significant at retention. If the included experiments are studying a true null hypothesis the expected distribution of p-values is uniform, represented by the dotted line. If the experiments are studying a true effect, the expected distribution becomes increasingly right skewed as a function of statistical power. The expected right skewed distribution associated with 33% power is plotted by the dashed line. The observed p-curve is plotted by the solid line and was substantially flatter than the 33% power distribution. The half p-curve analysis included p-values below p = .025 and was significantly right skewed. The right skew did not survive deletion of the most extreme value.

```{r fig5, echo = FALSE, fig.cap = "(ref:fig5-caption)", fig.align = "center", out.width="90%"}
knitr::include_graphics("../../figs/fig5.pdf")
```

# Sensitivity analyses

## Precision-effect with standard error (PEESE) model
When publication bias is present in a body of evidence, sample size and effect size can be negatively correlated [@Stanley2014-xl]. The PEESE model fits a quadratic relationship between effect size and standard error to reflect the intuition that publication bias is stronger for low precision studies than high precision studies. The rationale is that low precision studies need to overestimate effects to achieve significance and get published, while high precision studies can publish without exaggerated effects; thus, creating greater publication bias among lower precision studies [@Carter2019-vv; @Stanley2014-xl]. A weighted-least-squares regression model was fit with effect size regressed on the square of the standard error, weighted by the inverse of the variance:
$$g_{i} = b_{0} + b_{1}se_{i}^2 + e_{i}$$
The PEESE method estimated a non-significant benefit of self-controlled learning after controlling for publication bias, *g* = .054, *p* = .659.

## *P*-curve effect estimation
A *p*-curve model was fit to the overall retention effect size data, unlike the first primary *p*-curve which was fit to the reported significant results. The *p*-curve is a function of sample size and effect size, and because sample size is known, the effect size that provides the best fit to the observed *p*-curve can be estimated [@Simonsohn2014-xq]. A *p*-curve analysis conducted with th`R` package `dmetar` [@R-dmetar] was used to estimate the average effect size among the statistically significant effects in the meta-analysis. The model estimated an average effect of $g$ = .035.[^5] \textcolor{red}{The estimated power of included studies was 7\%, 95\% $CI$ [5\%, 22\%]. Unfortunately, \emph{p}-curve does not perform well in the presence of heterogeneity and these results should be interpreted cautiously.}

[^5]: The *p*-curve of effect sizes was significantly flatter than the expected 33% power curve as well, *p* = .009.

## \textcolor{red}{\emph{Z}-curve}
\textcolor{red}{A \emph{z}-curve was fit to the overall retention data and estimated the power of statistically significant studies (ERR) as 14\%, 95\% CI [5\%, 35\%]. The power of all studies conducted (EDR) was estimated as 6\%, 95\% $CI$ [5\%, 14\%]. The 95\% confidence intervals for both the ERR and EDR failed to include the observed discovery rate of 48\%, suggesting significant publication bias in the data.}

## Acquisition and transfer
In light of the evidence that experiments are apparently selected for positive self-controlled learning effects at retention, pre-planned exploratory estimates of the effect of self-controlled practice on acquisition and transfer performance \textcolor{red}{can no longer be considered reliable. However, given that some have argued that transfer tests are more senstive measures of motor learning than delayed retention tests} [@Chiviacowsky2002-ep; @Fairbrother2012-gz]\textcolor{red}{, the transfer test data were analzed via both naïve random effects and weight function models. The naïve estimate at transfer was $g$ = .52, while the bias corrected estimate was $g$ = .17, $p$ = .24. As with delayed retention, the selection model provided a better fit to the transfer data than the naïve model, $p$ = .008.} The primary take away from \textcolor{red}{these analyses} is that the reported self-controlled learning effects to date are unreliable.

# Discussion
The primary objective of this meta-analysis was to assess the effect of providing choices during the acquisition of a motor skill on delayed retention performance in the general population. A secondary objective was to test between motivation and informational explanations for self-controlled learning benefits by investigating whether choice-type moderates the effect of choice. To this aim, an extensive search for experiments that compared self-controlled practice to a yoked comparison group was conducted. Effect size and moderator data were ascertained from data reported in the research articles or, in some cases, received directly from the authors of the studies. Efforts were taken to ensure that each effect size calculation and moderator code could be reproduced by an independent party. In parallel, the results of published experiments that achieved a hypothesized statistically significant result in favour of self-control were extracted directly from the articles and outlined in a *p*-curve disclosure form (see Appendix A). Pre-registered primary analyses were applied to the data and results were followed up with a suite of sensitivity analyses.

The naïve random effects model estimated a benefit from self-controlled practice of *g* = .44. However, the naïve model fails to account for selection effects, such as publication bias and *p*-hacking, and as such overestimates the true average effect when these selection effects are present [@Carter2019-vv; @Hedges1996-yh; @Stanley2014-xl]. Publication status was a significant moderator of the self-controlled practice effect, accounting for 48% of the total heterogeneity in the model. Published experiments reported an average benefit of *g* = .54 while unpublished experiments reported no benefit at all on average. It is possible that researchers use statistical significance, typically defined as *p* < .05 on a two-tailed test, to filter their results for publication. To account for potential selection effects driven by statistical significance, a weight-function model was fit to the retention test effect size data with a one-tailed *p*-value cutpoint of .025 included in the model [@Vevea1995-uv]. The adjusted model provided a significantly better fit to the data than the naïve random effects model. The model estimated the selection-adjusted benefit of self-controlled learning as *g* = .11, a dramatic departure from the naïve estimate of *g* = .44. Two additional bias correction techniques were conducted to assess the sensitivity of this result to changes in correction methodology. The PEESE method estimated the effect at *g* = .05, while *p*-curve estimated *g* = .04, and neither analysis was able to rule out the null hypothesis.

In parallel to the meta-analysis described above, a *p*-curve was conducted on the reported significant results. The *p*-curve used somewhat different inclusion criteria focusing only on published, statistically significant results suggesting a self-controlled learning benefit. In addition, the *p*-curve included results reported for any dependent measure in an article, even if the focal measure (of this meta-analysis) was reported as non-significant. Therefore, the *p*-curve was more inclusive of evidence reported by authors as favouring a self-controlled benefit while ignoring experiments with null effects. The results revealed both significant right skew below *p* = .025 (two-tailed) and a *p*-curve that was significantly flatter than a distribution with an expected power of 33%. The evidence of right skew, indicating superiority of self-control relative to yoked conditions, was tenuous and did not survive the deletion of the most extreme result--an experiment that reported a benefit from self-control of *g* = 2.16 [@Wulf2014-ti]. The overall *p*-curve produced an estimate that the true power of the included experiments was 5%, leading to a rejection of the hypothesis that the experiments contained evidential value.

It appears from these analyses that the substantial self-controlled learning literature is, as of now, insufficient to provide evidence that self-controlled practice is more effective than a yoked practice. The bias correction techniques applied in this analysis are sensitive to unknown conditions, such as the true average effect size and the amount of true heterogeneity; although efforts were taken to provide coverage across most plausible conditions. The corrected estimates produced by the weight-function model, *p*-curve, and PEESE methods appeared to converge on trivially small effects. Further, the *p*-curve of significant results suggested a lack of evidential value. Based on the model performance parameters we \textcolor{red}{tested} [@Carter2019-vv], \textcolor{red}{which allowed up to .15 unites of error as acceptable performance, our results are consistent with a} self-controlled learning \textcolor{red}{benefit} ranging from $g$ = -.11 to .26, with \textcolor{red}{a} plausible upper \textcolor{red}{95\%} confidence limit of $g$ = .33. Thus, this analysis does not rule out the possibility that self-controlled practice provides meaningful motor learning benefits on average. The present literature, however, appears insufficient to establish that a self-control benefit indeed exists.

Turning to the current theoretical debates surrounding the motivational and informational underpinnings of self-controlled learning, these debates now seem moot, or at least premature. The effectiveness of self-control was not moderated by choice-type, suggesting that self-controlled practice may be ineffective regardless of the nature of the choices provided. Indeed, the only factor we tested that moderated the effect of self-controlled practice was publication status.

## Future studies
Given that the current meta-analysis failed to support the widely touted assertion of a substantial self-controlled learning benefit [@Sanli2013-xn; @Ste-Marie2019-bw; @Wulf2016-gf], considerations need to be given to the design and research practices for future studies. Registered reports provide one possible path forward [@Caldwell2020-jg]. A registered report involves submitting a research proposal to a two-phase peer-review. The first phase of the review occurs prior to data-collection and is assessed based on the proposed methodology, rationale, and potential contribution. If accepted in principle, researchers commit to carrying out the registered experiment and submitting the results in a final article for the second phase of peer-review. The final article is peer-reviewed for quality and adherence to the registered plan, but accept-reject decisions at this point are not based on the results. In theory, this practice should eliminate *p*-hacking and, for literatures composed entirely of registered reports, publication bias. A number of motor behaviour and/or kinesiology journals have begun adopting registered reports as an option for authors, including the *Human Movement Science*, *Frontiers in Movement Science and Sport Psychology*, *Journal of Sport and Exercise Psychology*, *Journal of Sport Sciences*, and *Reports in Sport and Exercise (formerly Registered Reports in Kinesiology)*.

While registered reports are a potentially fruitful process to begin the accumulation of evidence regarding self-controlled learning, there are practical issues with investigating self-controlled learning that motor learning researchers may find overly burdensome. For example, to have 80% power to detect an effect of *g* = .26 with a two cell experimental design, 506 participants are required. If the weight-function adjusted estimate of *g* = .11 is accurate, *N* = 2600 are required. More challenging still would be testing between hypothesized motivational and informational mechanisms. For example, if a 2 (choice) X 2 (choice-relevance) experiment were conducted to test whether the instructional-relevance of choice fully attenuates its effect, four times as many participants would be required to maintain the same degree of power [@Simonsohn2014-blog]. In contrast, the median sample size among experiments included in this meta-analysis was *N* = 36, which is typical of motor learning experiments in general [@Lohse2016-cf].

In addition to challenges with establishing that an effect exists, additional challenges will emerge if researchers are interested in generalizing the benefits of self-controlled practice beyond comparisons to a yoked group, as has been the case thus far [@Ste-Marie2019-bw; @Wulf2016-gf]. Yoking may allow for inferences to be made about the act of making certain choices, but it may not provide an adequate control group for evaluating best practices in an applied setting [e.g., @Barros2019-my; @Ste-Marie2019-bw; @Yantha2019-dt]. Indeed, given that our estimate suggests the advantage of self-controlled over yoked practice is small, if it exists at all, it seems unlikely that self-control would be more effective than an instructor-guided practice. An instructor-guided group could easily be argued to have advantages over a yoked group, because of the ability for the instructor to adapt choices to the current practice context and to make use of personal experience and expertise. Following this logic, experiments investigating the benefit of self-controlled over instructor-guided practice could conceivably require substantially larger samples than experiments that use yoked comparison groups.

## Exploratory analysis of pre-registered experiments
There have been, to our knowledge, four pre-registered experiments that have compared self-controlled and yoked practice [@Grand2017-de; @McKay2020-vj; @StGermain2021-lg; @Yantha2019-dt]. Three of these experiments failed to meet our inclusion criteria because they were not published or part of an accepted thesis at the time of the analysis [@McKay2020-vj; @StGermain2021-lg; @Yantha2019-dt]. These pre-registered experiments should provide estimates of the self-control effect unbiased by selection effects and are therefore more useful for estimating the real average effect than attempting to correct biased experiments after the fact [@Carter2019-vv]. A random effects model was used to estimate the average effect of self-control in the four experiments and yielded *g* = .02, 95% *CI* [-.17, .21]. These results converge with the bias-corrected estimates around trivially small differences between self-controlled and yoked practice conditions.

## Conclusion
We set out to assess the effect of self-controlled practice on motor learning. The published literature on the subject to date appeared unambiguously supportive of a self-control benefit, yet the results of this meta-analysis suggest this may not be the case. If authors, reviewers, and editors select for statistical significance when deciding if experiments get published, the published literature becomes biased [@Ioannidis2005-km]. Worse still, filtering based on statistical significance may well incentivize researchers to leverage researcher degrees of freedom to achieve a significant result, a practice known as *p*-hacking, further biasing the literature [@Wicherts2016-qz]. An instructive example of the potential impact of selection effects comes from research studying the so-called ego-depletion effect [@Baumeister2007-rb; @Hagger2010-cs]. In a typical study, participants are asked to engage in activities that supposedly drain a limited reservoir of willpower, termed ego-depletion, and are subsequently measured on a dependent measure requiring an additional exertion of self-control, such as a Stroop task. The typical finding is that performance suffers on the second task if ego-depletion occurs beforehand. A meta-analysis by Hagger and colleagues [-@Hagger2010-cs] reported the average effect of ego-depleting interventions on willpower dependent measures was *d* = .62. There was apparent consensus in the field that willpower relied on a limited resource due to the ostensibly unambiguous evidence in support of the theory [@Baumeister2016-df]. Nevertheless, when bias correction methods were applied in a meta-analysis of ego-depletion literature, the adjusted estimates often did not differ significantly from zero [@Carter2015-zk]. Subsequently, a pre-registered, multi-lab replication project tested a sample of *N* = 2141 and reported that the ego-depletion effect was close to zero [@Hagger2016-ay]. Thus, a prominent psychological construct substantiated by a large corpus of peer-reviewed evidence was investigated using cutting edge meta-analytic techniques that corrected for selection bias and the result was a trivially small estimated effect--an estimate supported by a subsequent large scale pre-registered replication effort. Notably, both the bias corrected meta-analysis and the subsequent multi-lab replication efforts have been criticized by ego-depletion theorists [@Cunningham2016-qy; @Baumeister2016-df]. Others have sharply challenged these critiques [@Schimmack2020-ea], and while debate continues among social psychologists about the underlying theory at stake [e.g., @Dang2018-xa], there is consensus that several methods shown to produce positive results in the past are unlikely to replicate in future experiments.

In stark parallel to the ego-depletion literature, the findings of the current research suggest the self-controlled motor learning literature may be similarly biased. As motor learning researchers consider the path forward for self-controlled learning, non-bias related limitations of the extant literature should be addressed. For example, yoked groups fail to isolate putative motivational and informational processes when self-controlling learners make choices pertinent to acquiring a skill [@Carter2016-fq; @Carter2017-mk; @Lewthwaite2015-bd]. Further, exclusive reliance on yoked comparison groups limits the generalizability of self-controlled learning to applied settings where the alternative to self-control is typically coach or instructor control (i.e., those with domain-specific knowledge). As motor learning researchers in this area move forward, they are faced with the question of whether this effect is worth the resources required to study it. If that answer is yes, then in addition to being pre-registered and an adequately powered design, future self-controlled learning experiments should provide insight about either the underlying processes at work or the real world usefulness of this practice variable.

# Contributions

Contributed to conception and design: BM, JH, ZY, MJC, DSM.

Contributed to acquisition of data: BM, JH, ZY, MJC.

Contributed to analysis and interpretation of data: BM.

Contributed to drafting and revisions of article: BM, JH, ZY, MJC, DSM.

# Acknowledgments

We would like to thank Heather Smith for her help with data extraction.

# Data, materials, and code availability
\label{sec:sharing}
All material, data, and scripts to reproduce our analyses and figures can be accessed here: https://osf.io/qbg69.

# R packages used in this project

`r cite_r("../r-references.bib")`.

# Conflict of interest
The authors declare no competing interests.

# Funding
BM was supported by a Social Sciences and Humanities Research Council (SSHRC) CGS-Doctoral Grant. MJC was supported by a Natural Sciences and Engineering Research Council (NSERC) of Canada Discovery Grant (RGPIN-2018-05589).

# References

```{=tex}
\begingroup
\interlinepenalty = 10000
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
References marked with an asterisk (*) indicate studies included in the meta-analysis.
<!-- <div id="refs" custom-style="Bibliography"></div> -->

\endgroup

```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_a.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_b.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_a.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_b.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_a.Rmd')
```


```{r echo = FALSE, results = 'asis', cache = FALSE}
papaja::render_appendix('appendix_b.Rmd')
```

